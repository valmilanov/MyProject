---
title: "GLDM 3001: Module 8 -  Machine Learning: Unsupervised Learning"
date: '`r Sys.Date()`'
output: html_notebook
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



```{r packages, warning = FALSE, message = FALSE}
library(ggplot2)
library(tidyverse)
library(dplyr)
library(graphics)
library(mdsr) 
library(utils)
library(rgdal)
library(fs)
```

## Objectives

* Apply estimation and testing methods

* Investigate relationships between two or more variables 

* Use Clustering, **Hierarchical clustering**, **k-means** 



> We will explore few techniques in **unsupervised learning**, where there is no response variable `y`. Here, we simply have a set of observations `X`, and we want to understand the relationships among them.

## Unsupervised vs Supervised Learning:

>	Previous sessions focus on supervised learning methods such as regression and classification.

- In that setting we observe both a set of features $X_1, X_2, . . . , X_p$ for each object, as well as a response or outcome variable $Y$ . The goal is then to predict $Y$ using $X_1, X_2, . . . , X_p$.

- Here the focus will be on unsupervised learning, where we observe only the features $X_1, X_2, . . . , X_p$ . We are not interested in prediction, because we do not have an associated response variable $Y$.

>	The goal is to discover interesting things about the measurements: 

- Is there an informative way to visualize the data? 

- Can we discover subgroups among the variables or among the observations?


## The Challenge of Unsupervised Learning

>	Unsupervised learning is more subjective than supervised learning, as there is no simple goal for the analysis, such as prediction of a response.

> Techniques for unsupervised learning are of growing importance in a number of fields:

- subgroups of breast cancer patients grouped by their gene expression measurements

- groups of shoppers characterized by their browsing and purchase histories

- movies grouped by the ratings assigned by movie viewers


## Advantages

> It is often easier to obtain **unlabeled data** — from a lab instrument or a computer — than **labeled data**, which can frequently require human intervention

- For example it is difficult to automatically assess the overall sentiment of a movie review: is it favorable or not?



## 1 Clustering Techniques


> **Clustering** refers to a very broad set of techniques for finding **subgroups**, or **clusters**, in the data.

> We seek a partition of the data into distinct groups so that the observations within each group are quite similar to each other.

> To make this concrete, we must define what it means for two or more observations to be **similar** or **different**.

> In general, this is often a domain-specific consideration that must be made based on knowledge of the data being studied.


#### Two clustering methods 

- 1	In **hierarchical clustering**, we do not know in advance how many clusters we want; in fact, we end up with a tree-like visual representation of the observations, called a dendrogram, that allows us to view at once the clusterings obtained for each possible number of clusters, from `1` to `n`.

- 2	In **K-means** clustering, we seek to partition the observations into a **pre-specified** number of clusters.



###	1.1 Hierarchical clustering


> Hierarchical clustering is an alternative approach which does not require that we commit to a particular choice of `K`, prespecifed number of clusters.

> Most common type of hierarchical clustering, and refers to the fact that a dendrogram is built starting from the leaves and combining clusters up to the trunk.


#### Approach description: Builds a hierarchy in a “bottom-up” fashion

> Start with each point in its own cluster

> Identify the closest two clusters and merge them

>	Repeat

> Ends when all points are in a single cluster


#### KEY TERMS FOR HIERARCHICAL CLUSTERING

- **Dendrogram**: A visual representation of the records and the hierarchy of clusters to which they belong.

- **Distance**: A measure of how close one record is to another.

- **Dissimilarity**: A measure of how close one cluster is to another.


When the description of an object consists of a set of numerical variables (**none** of which is a **response**), there are two main steps in constructing a tree to describe the relationship among the cases in the data:

> 1. Represent each case as a point in a Cartesian space. 

> 2. Make branching decisions based on how close together points or clouds of points are.


### Vehicle clustering example** 

For illustration, consider the unsupervised learning process of identifying different types of cars. The United States Department of Energy maintains automobile characteristics for thousands of cars: `miles per gallon`, `engine size`, `number of cylinders`, `number of gears`, etc. 

Please see their guide for more information. Here, we download a ZIP file from their website that contains fuel economy rating for the 2016 model year.

Next, we use the `readxl` package to read this file into R, clean up some of the resulting variable names, select a small subset of the variables, and filter for distinct models of Toyota vehicles. The resulting data set contains information about `75` different models that Toyota produces.

**Note: IMPORTANT** Uncomment the code below to download and unzip the data file. It prompts you to unzip and it will create  folder by the name `16data` in your working directory. You need to run it only once. Make sure that you set up your Set Your Working directory to Source File Location from the Session Tab on the Menu.


```{r data download}

# src <- "https://www.fueleconomy.gov/feg/epadata/16data.zip"
# lcl <- usethis::use_zip(src)

```


Next, we use the `readxl` package to read this file into `R`, clean up some of the resulting variable names, select a small subset of the variables, and filter for distinct models of Toyota vehicles. The resulting data set contains information about `75` different models that Toyota produces.

```{r Models}

# load the readxl package to read the xlsx file in R
library(readxl)

# set the working directory to the current one, where you downloaded the file
work_dir <- getwd()
setwd(work_dir)

filename <- fs::dir_ls("16data", regexp = "public\\.xlsx") %>% head(1)

# use read_excel function to read the file by creating the full path to it, using paste0 function.


cars <- read_excel(filename) %>%
  janitor::clean_names() %>%
  dplyr::select(
    make = mfr_name,
    model = carline,
    displacement = eng_displ,
    number_cyl,
    number_gears,
    city_mpg = city_fe_guide_conventional_fuel,
    hwy_mpg = hwy_fe_guide_conventional_fuel
  ) %>%
  distinct(model, .keep_all = TRUE) %>%
  filter(make == "Toyota") # filter Toyota vehicles only

# have a look at the data
glimpse(cars)

# see the first few observation
# head(cars)
```

> Toyota has a diverse lineup of cars, trucks, SUVs, and hybrid vehicles. 

> Can we use unsupervised learning to categorize these vehicles in a sensible way with only the data we have been given?

> For an individual quantitative variable, it is easy to measure how far apart any two cars are: one can take the difference between the numerical values.

> The different variables are, however, on different scales and in different units. 

- For example, `gears` ranges only from `1` to `8`, while `city_mpg` goes from `13` to `58`. This means that some decision needs to be made about rescaling the variables so that the differences along each variable reasonably reflect how different the respective cars are. 

> There is more than one way to do this, and in fact, there is no universally “best” solution—the best solution will always depend on the data and your domain expertise. 

> The `dist()` function takes a simple and pragmatic point of view: **Each variable is equally important**.

The output of `dist()` gives the distance from each individual car to every other car.

```{r}

car_diffs <- cars %>%
  column_to_rownames(var = "model") %>% #  convert the second column from cars (model) to row names (from tibble package) 
  dist() 

str(car_diffs) # this is dist object

```

Create distance matrix object from the `car_diffs` - `dist` object.

```{r}
# convert the 
car_mat <- car_diffs %>% 
  as.matrix()

car_mat[1:6, 1:6] %>% # try 10 instead of 6
  round(digits = 2)
```



> The above point-to-point distance matrix is analogous to the tables that used to be printed on road maps giving the distance from one city to another. 

- Notice that the distances are symmetric: It is the same distance from Boston to Los Angeles as from Los Angeles to Boston (3,036 miles, according to the table).

> Knowing the distances between the cities is not the same thing as knowing their locations. But the set of mutual distances is enough information to reconstruct the relative positions of the cities.

> Cities, of course, lie on the surface of the earth. 

That is not true for the **distance** between automobile types. 

> The set of mutual distances provides information equivalent to knowing the relative positions of these cars in a `p`-dimensional space. 

> This can be used to construct branches between nearby items, then to connect those branches, and so on until an entire tree has been constructed. 

> The process is called **hierarchical clustering**. Figure below shows a tree constructed by hierarchical clustering that relates Toyota car models to one another.

```{r clustering, fig.height = 14, fig.width = 8}
#install if not installed
# install.packages("ape")

library(ape) 
car_diffs %>% 
  hclust() %>% 
  as.phylo() %>% 
  plot(cex = 1.2, label.offset = .5) # adjust the cex to better visualize

```

Description of R-functions used:

* `hclust()` Hierarchical cluster analysis on a set of dissimilarities and methods for analyzing it.

* `as.phylo()` is a generic function which converts an object into a tree of class "phylo". 

> There are many ways to graph such trees, but here we have borrowed from biology by graphing these cars as a phylogenetic tree (diagram that represents evolutionary relationships among organisms). 

> Careful inspection of Figure above reveals some interesting insights. 

- The first branch in the tree is evidently between hybrid vehicles and all others. 

This makes sense, since hybrid vehicles use a fundamentally different type of power to achieve considerably better fuel economy. 

> Moreover, the first branch among conventional cars divides large trucks and SUVs (e.g., Sienna, Tacoma, Sequoia, Tundra, Land Cruiser) from smaller cars and cross-over SUVs (e.g., Camry, Corolla, Yaris, RAV4). 

We are confident that the gearheads in the readership will identify even more subtle logic to this clustering. One could imagine that this type of analysis might help a car-buyer or marketing executive quickly decipher what might otherwise be a bewildering product line.


###	1.2 K-means clustering (Geospatial data example)

> Another way to group similar cases is to assign each case to one of several distinct groups, but without constructing a hierarchy. 

> The output is not a tree but a choice of group to which each case belongs. (There can be more detail than this; for instance, a probability for each group that a specific case belongs to the group.) 

> This is like classification except that here **there is no response variable**. 

> Thus, the definition of the groups must be inferred implicitly from the data.

#### K-means algorithm description:

1.	Randomly assign a number, from `1` to `K`, to each of the observations. These serve as initial cluster assignments for the observations

2.	Iterate until the cluster assignments stop changing:

+  2.1	For each of the `K` clusters, compute the cluster centroid. The `k`th cluster centroid is the vector of the p feature means for the observations in the `k`th cluster.

+  2.2	Assign each observation to the cluster whose centroid is closest (where closest is defined using Euclidean distance).


#### KEY TERMS FOR K-MEANS CLUSTERING

* **Cluster**: A group of records that are similar.

* **Cluster mean**: The vector of variable means for the records in a cluster.

* `K`: The number of clusters. (**must be specified**)

**K-Means Clustering** is a technique to divide data into different groups, where the records in each group are similar to one another. A goal of clustering is to identify significant and meaningful groups of data. The groups can be used directly, analyzed in more depth, or passed as a feature or an outcome to a predictive regression or classification model.

The K-means approach, like many clustering methods, is **highly algorithmic** (can’t be summarized in a formula) and is iterative. The basic idea is that you are trying to find the centroids of a fixed number of clusters of points in a high-dimensional space. 

In two dimensions, you can imagine that there are a bunch of clouds of points on the plane and you want to figure out where the centers of each one of those clouds is.

`K-means` divides the data into `K` clusters by minimizing the sum of the squared distances of each record to the `mean` of its assigned cluster. The is referred to as the `within-cluster sum of squares` or `within-cluster SS`. K-means does not ensure the clusters will have the same size, but finds the clusters that are the best
separated.

**Reference:** Read more at [K-means](https://bookdown.org/rdpeng/exdata/k-means-clustering.html#)

#### Geospatial data example:

Consider the cities of the world (in `world_cities`, in the `mdsr` package). Cities can be different and similar in many ways: population, age structure, public transportation and roads, building space per person, etc. The choice of *features* (or variables) depends on the purpose you have for making the grouping.

Our purpose is to show you that clustering via machine learning can actually identify genuine patterns in the data. 

We will choose features that are utterly familiar: the **latitude** and **longitude** of each city.

You already know about the location of cities. They are on land. And you know about the organization of land on earth: most land falls in one of the large clusters called continents.

But the `WorldCities` data doesn’t have any notion of continents. Perhaps it is possible that this feature, which you long ago internalized, can be learned by a computer that has never even taken grade-school geography.

For simplicity, consider the 4,000 biggest cities in the world and their longitudes and latitudes.

```{r selectCities}

big_cities <- world_cities %>% 
  arrange(desc(population)) %>%
  head(4000) %>% 
  dplyr::select(longitude, latitude)
glimpse(big_cities)

```

Note that in these data, there is no ancillary information—not even the name of the city. However, the `k-means` clustering algorithm will separate these `4,000` points—each of which is located in a two-dimensional plane—into `k` clusters based on their locations alone.

```{r warning=FALSE}

set.seed(15)
# install the package first if not installed
#install.packages("mclust")
library(mclust) 

# form 6 cluster iteratively
city_clusts <- big_cities %>% 
  kmeans(centers = 6, nstart =10) %>% 
  fitted("classes") %>% 
  as.character()

# form 6 cluster iteratively, by forming initially 10 random sets
km <- kmeans(big_cities, centers = 6, nstart = 10)

# inspect the structure of the kmeans output cluster object
str(km)

# access two important features of cluster, their size and centers
km$size
km$centers



# add a variable to record the cluster to which a point belongs to
big_cities <- big_cities %>% 
  mutate(cluster = city_clusts)

# graph the clusters, using the cluster variable to pick the color
big_cities %>% 
  ggplot(aes(x = longitude, y = latitude)) +
  geom_point(aes(color = cluster), alpha = 0.5)  + 
  scale_color_brewer(palette = "Set2")

```

What did the clustering algorithm seem to have identified?

### Projections

The Earth happens to be an oblate spheroid—a three-dimensional flattened sphere. Yet we would like to create two-dimensional representations of the Earth that fit on pages or computer screens. The process of converting locations in a three-dimensional *geographic coordinate system* to a two-dimensional representation is called *projection*.


> A *coordinate reference system (CRS)* is needed to keep track of geographic locations. 

> Every spatially-aware object in `R` can have a projection string, encoded using the `PROJ.4` map projection library. 

> These can be retrieved (or set) using the `proj4string()` command.

There are many *CRS*s, but a few are most common. A set of EPSG (European Petroleum Survey Group) codes provides a shorthand for the full PROJ.4 strings. The most commonly-used are:

**EPSG:4326** Also known as WGS84, this is the standard for GPS systems and Google Earth.

**EPSG:3857** A Mercator projection used in maps tiles4 by Google Maps, Open Street Maps, etc.

**EPSG:4269** NAD83, most commonly used by U.S. federal agencies. 



`R-Code` below uses **EPSG:4326**. Use the other **two standards** **EPSG:3857**, **EPSG:4269**

Use `K-means` algorithm for each of the three (3) projections above and compare the three projections to the standard cartesian coordinates used in the example. Which one is best in identifying the continents?


#### K-means results in **EPSG:4326** (WGS84) CRS


```{r warning = FALSE}
library(rgdal)

# assign the big_cities data.frame to a working data.frame object df 
df <- big_cities 

# create spatial object from d
coordinates(df) <- 1:2

# Set WGS 84 (EPSG:4326) standard for projecting longitude latitude coordinates
proj4string(df) <- CRS("+init=epsg:4326")

# coordinate reference system using the EPSG:4326 standard
CRS.new <- CRS("+init=epsg:4326")

# the d object in the new CRS, you may print out few records to see how it looks in the new CRS
df.new <- spTransform(df, CRS.new)

# just for information review the 
proj4string(df.new) %>% strwrap()


# form 6 cluster iteratively
city_clusts <- as.data.frame(df.new) %>%
kmeans(centers = 6) %>% fitted("classes") %>% as.character()

# add a variable for the newly formed clusters
df.new <- as.data.frame(df.new) %>% mutate(cluster = city_clusts) 

# graph the clusters, using the cluster variable to pick the color
df.new %>% ggplot(aes(x = longitude, y = latitude)) +
geom_point(aes(color = cluster), alpha = 0.5) +
scale_color_brewer(palette = "Set3")


```


#### k-means in **EPSG:4269** (NAD83) CRS.

```{r warning = FALSE}

# Set NAD83 (EPSG:4269) standard for projecting longitude latitude coordinates
proj4string(df) <- CRS("+init=epsg:4269")

# coordinate reference system using the EPSG:4326 standard
CRS.new <- CRS("+init=epsg:4269")

# the d object in the new CRS, you may print out few records to see how it looks in the new CRS
df.new <- spTransform(df, CRS.new)

# just for information review the 
proj4string(df.new) %>% strwrap()


# form 6 cluster iteratively
city_clusts <- as.data.frame(df.new) %>%
kmeans(centers = 6) %>% fitted("classes") %>% as.character()

# add a variable for the newly formed clusters
df.new <- as.data.frame(df.new) %>% mutate(cluster = city_clusts) 

# graph the clusters, using the cluster variable to pick the color
df.new %>% ggplot(aes(x = longitude, y = latitude)) +
geom_point(aes(color = cluster), alpha = 0.5) +
scale_color_brewer(palette = "Set3")

```